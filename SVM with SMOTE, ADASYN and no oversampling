import pandas as pd
from tsfresh.feature_selection.relevance import calculate_relevance_table
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import roc_auc_score, average_precision_score, classification_report
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE, ADASYN
from sklearn.metrics import make_scorer

# Function to select top n TSFresh features
def select_top_n_features(X, y, n_top_features):
    relevance_table = calculate_relevance_table(X, y, ml_task='classification')
    sorted_table = relevance_table.sort_values(by='p_value')
    top_n_features = sorted_table.head(n_top_features)['feature']
    return X[top_n_features], top_n_features


def main():
    # Load dataset
    file_path = r"E:\Desktop\Machine Intelligence\combined extracted features with headers\combined_aligned_features.csv"
    data = pd.read_csv(file_path)
    
    # Drop unwanted columns (RecID, Pair_RecID)
    data = data.drop(columns=["RecID", "Pair_RecID"])
    
    # Encode categorical columns (Placental_position to 0 or 1, RecType is the target)
    label_encoders = {}
    for column in ['Placental_position', 'RecType']:
        le = LabelEncoder()
        data[column] = le.fit_transform(data[column])
        label_encoders[column] = le
    
    # Define obstetric features explicitly
    obstetric_features = ['Gestation', 'Rectime', 'Age', 'Weight', 'Placental_position', 'Height', 'Newborn_weight']
    
    # Define features (X) and target (y)
    X = data.drop(columns=['RecType'])  # Features
    y = data['RecType']  # Target
    
    # Identify TSFresh features as any column that is NOT in obstetric_features
    tsfresh_features = [col for col in X.columns if col not in obstetric_features]
    
    # TSFresh feature count to select
    n_top_features = 100
    
    # Define scalers to test
    scalers = {
        'StandardScaler': StandardScaler()
    }
    
    # SVM hyperparameter grid
    param_grid_svm = {
        'C': [0.1],
        'kernel': ['linear'],
        'gamma': ['scale'],
        'degree': [2]
    }
    
    # Stratified K-Folds cross-validation
    strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    # Scoring metrics for GridSearchCV
    scoring = {
        'AUC': make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr'),
        'Average Precision': make_scorer(average_precision_score, needs_proba=True, average='macro')
    }
    
    # List of random states to iterate over
    random_states = [42, 2, 4, 11, 30]
    
    # Iterate over different random states
    for random_state in random_states:
        print(f"Running with random_state: {random_state}")
    
        # Select top TSFresh features
        X_tsfresh = X[tsfresh_features]
        X_tsfresh_selected, top_tsfresh_features = select_top_n_features(X_tsfresh, y, n_top_features)
    
        # Combine the obstetric features with the selected TSFresh features
        X_selected = pd.concat([X[obstetric_features], X_tsfresh_selected], axis=1)
    
        # Stratified train-test split to ensure class distribution in both sets
        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=random_state, stratify=y)
    
        # Run once with each scaler
        for scaler_name, scaler in scalers.items():
            print(f"\nUsing scaler: {scaler_name}")
            
            # List to store results
            results = []
    
            # Apply SMOTE oversampling
            smote = SMOTE(random_state=random_state)
            X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
            X_train_smote_scaled = scaler.fit_transform(X_train_smote)
            X_test_scaled = scaler.transform(X_test)
    
            # Initialize and fit SVM with GridSearch and SMOTE data
            svm_model = SVC(probability=True, random_state=random_state)
            grid_search = GridSearchCV(svm_model, param_grid_svm, cv=strat_kfold, scoring=scoring, refit='AUC', n_jobs=-1, verbose=1)
            grid_search.fit(X_train_smote_scaled, y_train_smote)
    
            # Get results for SMOTE
            y_pred_proba = grid_search.best_estimator_.predict_proba(X_test_scaled)
            auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
            avg_precision = average_precision_score(y_test, y_pred_proba, average="macro")
            y_pred = grid_search.best_estimator_.predict(X_test_scaled)
            report = classification_report(y_test, y_pred)
            results.append(("SMOTE", auc, avg_precision, report))
    
            # Apply ADASYN oversampling
            adasyn = ADASYN(random_state=random_state)
            X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)
            X_train_adasyn_scaled = scaler.fit_transform(X_train_adasyn)
            grid_search.fit(X_train_adasyn_scaled, y_train_adasyn)
    
            # Get results for ADASYN
            y_pred_proba = grid_search.best_estimator_.predict_proba(X_test_scaled)
            auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
            avg_precision = average_precision_score(y_test, y_pred_proba, average="macro")
            y_pred = grid_search.best_estimator_.predict(X_test_scaled)
            report = classification_report(y_test, y_pred)
            results.append(("ADASYN", auc, avg_precision, report))
    
            # Run without any oversampling
            X_train_scaled = scaler.fit_transform(X_train)
            grid_search.fit(X_train_scaled, y_train)
    
            # Get results without oversampling
            y_pred_proba = grid_search.best_estimator_.predict_proba(X_test_scaled)
            auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
            avg_precision = average_precision_score(y_test, y_pred_proba, average="macro")
            y_pred = grid_search.best_estimator_.predict(X_test_scaled)
            report = classification_report(y_test, y_pred)
            results.append(("No Oversampling", auc, avg_precision, report))
    
            # Print the results for each method
            for method, auc, avg_precision, report in results:
                print(f"\nMethod: {method} - AUC: {auc:.4f}, Average Precision: {avg_precision:.4f}")
                print("Classification Report:\n", report)
                print("=" * 60)

# Run the main function
if __name__ == "__main__":
    main()
